师姐的建议：

面试一共有三面 第一面比较重要

问的自己做的项目、dtw、动归

还问了梯度消失？

比较看面试官，当时我的面试官就是我部门的leader和导师，他们主要就是问项目

二面是我们大部门的cto

问了项目，然后问了一道算法题，好像是哈希，问了下冲突怎么处理



**DTW算法**

将序列某个时刻的点跟另一时刻多个连续时刻的点相对应就叫时间规整。

动态即动态规划的意思

DTW算法的步骤：

1.计算两个序列各个点之间的距离矩阵

2.寻找一条从矩阵的左上角到右下角元素和最小的路径

初始化

![[公式]](https://www.zhihu.com/equation?tex=L_%7Bmin%7D%281%2C+1%29+%3D+M%281%2C+1%29)

转态转移方程

![[公式]](https://www.zhihu.com/equation?tex=L_%7Bmin%7D%28i%2C+j%29+%3D%5Cmin%5C%7B+L_%7Bmin%7D%28i%2C+j-1%29%2C+L_%7Bmin%7D%28i-1%2C+j%29%2C+L_%7Bmin%7D%28i-1%2C+j-1%29%5C%7D+%2B+M%28i%2Cj%29)



**傅里叶变换，Constant Q**



**Transformer 和Music Transformer项目**

1.位置编码

1, encoder,decoder 结构，输入要先embedding，然后再加位置参数。encoder的输出作为decoder 的k和v输入

**小提琴陪练APP项目**

**ICASSP论文投稿**

### 1.在合理范围内，增大Batch_Size有何好处？

1. 内存利用率提高了，大矩阵乘法的并行化效率提高。
2. 跑完一次 epoch（全数据集）所需的迭代次数减少，对于相同数据量的处理速度进一步加快。
3. 在一定范围内，一般来说 Batch_Size 越大，其确定的下降方向越准，引起训练震荡越小。

### 2.盲目增大 Batch_Size 有何坏处？

1. 内存利用率提高了，但是内存容量可能撑不住了。
2. 跑完一次 epoch（全数据集）所需的迭代次数减少，要想达到相同的精度，其所花费的时间大大增加了，从而对参数的修正也就显得更加缓慢。
3. Batch_Size 增大到一定程度，其确定的下降方向已经基本不再变化。

### 3. 调节 Batch_Size 对训练效果影响到底如何？

1. Batch_Size 太小，模型表现效果极其糟糕(error飙升)。
2. 随着 Batch_Size 增大，处理相同数据量的速度越快。
3. 随着 Batch_Size 增大，达到相同精度所需要的 epoch 数量越来越多。
4. 由于上述两种因素的矛盾， Batch_Size 增大到某个时候，达到时间上的最优。
5. 由于最终收敛精度会陷入不同的局部极值，因此 Batch_Size 增大到某些时候，达到最终收敛精度上的最优。 

#### 正则化

正则化（Regularization）是机器学习中一种常用的技术，其主要目的是控制模型复杂度，减小过拟合。最基本的正则化方法是在原目标（代价）函数 中添加惩罚项，对复杂度高的模型进行“惩罚”。其数学表达形式为：

![[公式]](https://www.zhihu.com/equation?tex=%5Ctilde%7BJ%7D%5Cleft%28+w%3BX%2Cy+%5Cright%29+%3DJ+%5Cleft%28+w%3BX%2Cy+%5Cright%29%2B%5Calpha%5COmega%5Cleft%28+w+%5Cright%29)



